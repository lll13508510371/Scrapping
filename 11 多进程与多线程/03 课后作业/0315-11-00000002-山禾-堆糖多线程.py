"""
	ç›®æ ‡ç½‘å€ï¼šhttps://www.duitang.com/

	ä½œä¸šæè¿°ï¼šè¯·åœ¨ç½‘é¡µæœ€ä¸Šé¢æœç´¢æ¡†è¾“å…¥å…³é”®å­— â€œèœ¡ç¬”å°æ–°â€ è¿›è¡Œæœç´¢å›¾ç‰‡ï¼Œæ ¹æ®æœç´¢åˆ°çš„ç»“æœé‡‡é›†å‰åé¡µå›¾ç‰‡

	ä½œä¸šè¦æ±‚ï¼šç”¨å¤šè¿›ç¨‹åµŒå¥—å¤šçº¿ç¨‹çš„æ–¹å¼å®ç°
"""
import os
import threading
# import concurrent.futures
import concurrent.futures
import time

import requests


def send_requeset(query_param):
    headers = {
        'referer': 'https://www.duitang.com/search/?kw=èœ¡ç¬”å°æ–°&type=feed',
        'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36'
    }
    url = 'https://www.duitang.com/napi/blogv2/list/by_search/'

    # headersä¹Ÿä¸èƒ½ä¹±åŠ  --> UnicodeEncodeError: 'latin-1' codec can't encode characters in position 35-38: ordinal not in range(256) è¿™é‡ŒåŠ headerså°±æ˜¯è¿™æ ·
    response = requests.get(url=url, params=query_param)

    return response


def parse_data(response):
    total_data = response.json()

    obj_list = total_data['data']['object_list']

    return obj_list


def save_img(img_url):
    # lock = threading.Lock()
    try:
        img_content = requests.get(img_url, timeout=5).content
        img_name = img_url.split('/')[-1]
        # with lock:  -->  ä¸éœ€è¦ä¸Šé”ï¼Œæ¯ä¸€ä¸ªæ–‡ä»¶çš„åå­—éƒ½ä¸åŒï¼Œåªæœ‰æ“ä½œç›¸åŒæ–‡ä»¶çš„æ—¶å€™éœ€è¦ğŸ”’
        with open(os.path.join('/Users/lujinghan/PycharmProjects/Scrapping/11 å¤šè¿›ç¨‹ä¸å¤šçº¿ç¨‹/03 è¯¾åä½œä¸š/img', img_name),
                  mode='wb') as f:
            f.write(img_content)
            print(f'{img_name}ä¸‹è½½æˆåŠŸ')

    except Exception:
        pass


def main(query_param):
    time.sleep(1)
    response = send_requeset(query_param)

    obj_list = parse_data(response)

    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as T_executor:
        for obj in obj_list:
            img_url = obj['photo']['path']
            T_executor.submit(save_img, img_url)


if __name__ == '__main__':
    if not os.path.exists('/Users/lujinghan/PycharmProjects/Scrapping/11 å¤šè¿›ç¨‹ä¸å¤šçº¿ç¨‹/03 è¯¾åä½œä¸š/img'):
        os.mkdir('/Users/lujinghan/PycharmProjects/Scrapping/11 å¤šè¿›ç¨‹ä¸å¤šçº¿ç¨‹/03 è¯¾åä½œä¸š/img')

    start_time = time.time()

    with concurrent.futures.ProcessPoolExecutor(max_workers=5) as P_executor:
        for i in range(0, 24 * 50 + 1, 24):
            query_str_param = {
                'kw': 'èœ¡ç¬”å°æ–°',
                'after_id': str(i),
                'type': 'feed',
                'include_fields': 'top_comments,is_root,source_link,item,buyable,root_id,status,like_count,like_id,sender,album,reply_count,favorite_blog_id',
                '_type': '',
                '_': str(int(time.time() * 1000))  # ä¸æ£€æŸ¥æ—¶é—´æˆ³ï¼Œç½‘å€è¾“å…¥éšä¾¿è¾“å…¥ä¸€ä¸ªæ—¶é—´æˆ³å­—ç¬¦ä¸²éƒ½èƒ½è·å¾—å€¼
            }

            '''ï¼ï¼ï¼ï¼ä¹‹å‰è¿™é‡Œé€»è¾‘æœ‰é—®é¢˜ï¼ŒæŠŠè¿›ç¨‹æ± å¯¹è±¡å†™è¿›å¾ªç¯ä¸­äº†ï¼Œè™½ç„¶æœ€ç»ˆå¾—åˆ°äº†ï¼Œä½†è¿™æ ·æ¯ä¸€æ¬¡å¾ªç¯éƒ½åªæœ‰ä¸€ä¸ªè¿›ç¨‹åœ¨è¿è¡Œï¼Œä¸‹ä¸€æ¬¡å¾ªç¯åˆæ˜¯æ–°çš„è¿›ç¨‹æ± å¯¹è±¡ï¼Œç„¶ååªæäº¤å’Œè¿è¡Œä¸€ä¸ªè¿›ç¨‹'''
            # with concurrent.futures.ProcessPoolExecutor(max_workers=5) as P_executor:
            P_executor.submit(main, query_str_param)

    end_time = time.time()
    # å¯ä»¥è®°å½•æ± å­çš„è¿è¡Œæ—¶é—´
    print('è¿›ç¨‹æ± è¿è¡Œäº†ï¼š', end_time - start_time, 'ç§’')
    # query_str_param = {
    #     'kw': 'èœ¡ç¬”å°æ–°',
    #     'after_id': '24',
    #     'type': 'feed',
    #     'include_fields': 'top_comments,is_root,source_link,item,buyable,root_id,status,like_count,like_id,sender,album,reply_count,favorite_blog_id',
    #     '_type': '',
    #     '_': str(int(time.time() * 1000))
    # }
    # main(query_str_param)
'''
æˆ‘è®¾ç½®è®¾ç½®è¿›ç¨‹æ•°17ï¼ˆ2xæ ¸å¿ƒ+1) æˆ–15(2xæ ¸å¿ƒ-1)ï¼ˆæ­£å¸¸çš„æœ€ä¼˜è¿›ç¨‹æ•°ï¼Œç½‘ä¸Šè¯´æ³•ä¸ä¸€ï¼Œè¿˜æœ‰1ï¼š1ï¼‰ï¼Œ çº¿ç¨‹80ï¼ˆ10xçº¿ç¨‹ æ­£å¸¸çš„æœ€ä¼˜çº¿ç¨‹æ•°ï¼‰çˆ¬åˆ°ä¸€åŠå¡ä½äº†ï¼Œä¸çŸ¥é“ä»€ä¹ˆæƒ…å†µï¼Œå¯èƒ½æ˜¯æœåŠ¡å™¨è¯†åˆ«åˆ°äº†ä½ çš„ipåŒæ—¶è¿‡å¤šè¯·æ±‚ç»™åæ‰’äº†å§ï¼ˆåªæ˜¯æŸ¥è¯¢å‚æ•°ä¸ä¸€æ ·ï¼Œè¯·æ±‚çš„urlæ˜¯ä¸€æ ·çš„ï¼Œä¹Ÿå°±æ˜¯åŒä¸€ä¸ªæœåŠ¡å™¨é‡Œé¢æ‰¾å†…å®¹ï¼‰
éƒ½è®¾ç½®æˆ5åšä½œä¸šå½“å¤©éƒ½èƒ½çˆ¬åˆ°ï¼Œä½†ä»Šå¤©å¿«çˆ¬å®Œçš„æ—¶å€™å¡ä½äº†ï¼Œä½†æ¯”ä¸Šé¢çš„æƒ…å†µçˆ¬çš„å¤š
ç„¶åæ²¡åŠæ³•ï¼Œåªèƒ½è®¾ç½®try catch ä¼°è®¡æœ‰äº›å›¾ç‰‡è¯·æ±‚ä¸åˆ°æˆ–è€…æš‚æ—¶è¯·æ±‚ä¸åˆ°æˆ–è€…è¢«åæ‰’ï¼ˆåªæ˜¯æŸ¥è¯¢å‚æ•°ä¸ä¸€æ ·ï¼Œè¯·æ±‚çš„urlæ˜¯ä¸€æ ·çš„ï¼Œä¹Ÿå°±æ˜¯åŒä¸€ä¸ªæœåŠ¡å™¨é‡Œé¢æ‰¾å†…å®¹ï¼‰ --> img_content = requests.get(img_url, timeout=5).content
'''
