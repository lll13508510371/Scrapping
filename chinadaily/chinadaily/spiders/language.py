import re

import scrapy
from ..items import ChinadailyItem


class LanguageSpider(scrapy.Spider):
    name = "language"
    allowed_domains = ["chinadaily.com.cn"]

    # start_urls = [f"http://language.chinadaily.com.cn/thelatest/page_{i}.html" for i in range(1, 11)]

    # start_urls = ["http://language.chinadaily.com.cn/thelatest/page_1.html"]

    '''
    ï¼ï¼ï¼ï¼start_requests -->> è¿™ä¸ªå‡½æ•°ä¸æ˜¯è‡ªå®šä¹‰çš„å‡½æ•°ï¼Œscrapyæ¡†æ¶ä¼šåœ¨åº•å±‚å¯¹åº”æ‰¾ ä¹‹å‰å†™æˆ start_requestå°±çˆ¬ä¸åˆ°æ•°æ®è¿˜ä¸çŸ¥é“ä»€ä¹ˆæƒ…å†µ
    -->>   start_requests æ˜¯scrapyæ¡†æ¶é‡Œé¢çš„å‡½æ•°ï¼Œåªä¸è¿‡éœ€è¦è‡ªå·±æ‰‹åŠ¨åˆ›å»ºå†™é€»è¾‘ï¼Œæ¡†æ¶ä¼šå¤„ç†è¿™ä¸ªå‡½æ•°
    '''

    def start_requests(self):
        yield scrapy.Request(url="http://language.chinadaily.com.cn/thelatest/page_1.html", callback=self.parse)

    #     '''
    #     callback --> æŠŠè¿™ä¸€æ¬¡urlçš„è¯·æ±‚äº¤ç»™è°æ¥å¤„ç†  è¿™é‡Œäº¤ç»™parseå‡½æ•°æ¥å¤„ç†
    #     å›æ‰å’Œé€’å½’æœ‰ç‚¹åƒï¼Œç°åœ¨è¿˜ä¸èƒ½å‘ç°ä¸¤è€…çš„åŒºåˆ«
    #     !!! scrapyæœ‰å¾ˆå¤šcallback
    #     '''

    def parse(self, response):
        divs = response.css('.content_left>.gy_box')
        '''
        parseå‡½æ•°å·²ç»æ ¹æ®å‰é¢çš„urlè¯·æ±‚å¾—åˆ°äº†response
        '''

        # ä¸€é¡µ15ä¸ªæ•°æ®ï¼Œåé¡µ150
        for div in divs:
            title = div.css('.gy_box_txt>p:nth-child(1)>a::text').get().replace('"', "'")
            '''
            ï¼ï¼ï¼å†™å…¥csvæ–‡ä»¶æ—¶å€™å¾ˆå¤šç¬¦å·ä¸èƒ½å†™è¿›å»ï¼Œçœ‹æ–‡ä»¶å‘ç°æœ‰ä¸€äº›åœ°æ–¹è«åå¥‡å¦™æœ‰æ¢è¡Œå¯¼è‡´fieldå†™é”™ï¼Œ
            æ„è¯†åˆ°æ˜¯introductionçˆ¬å–åˆ°çš„ä¸€äº›æ•°æ®ç»“å°¾æœ‰ç©ºç™½ï¼Œç”¨strip()å»é™¤å°±æ²¡é—®é¢˜äº†ï¼Œå…¶å®ƒæœ‰é—®é¢˜çš„åœ°æ–¹å†replaceä¸€ä¸‹
            '''

            '''
            æ¡†æ¶ä¸èƒ½ç”¨å¼‚å¸¸æ•è·ï¼Œå¯èƒ½è¿˜æ˜¯ä¼šæŠŠé”™è¯¯æš´éœ²å‡ºæ¥ --->>>  ä¹Ÿå¯ä»¥å‘ç°ï¼Œå½“start_urlsæ„å»ºäº†å¤šä¸ªurlsï¼ŒæŸä¸€ä¸ªå‡ºé”™äº†ä¸ä¼šå½±å“å…¶å®ƒçš„è¯·æ±‚
                                                        åªä¼šç»™å‡ºé‚£ä¸ªé”™è¯¯è¯·æ±‚çš„é”™è¯¯ä¿¡æ¯
            è¿™é‡ŒåŠ äº†try except è¿˜æ˜¯ç»™å‡ºäº† introduction çš„é”™è¯¯ï¼Œç„¶åç»“æŸäº†ç¨‹åºè¿è¡Œï¼Œå¾—ç”¨if elseæ”¹ä¸€ä¸‹  
            ï¼ï¼ï¼çŸ¥é“ introduction æ˜¯æœ‰é—®é¢˜çš„ ä¸‹é¢ç»™å‡ºäº†introductionçš„æŠ¥é”™è¯´æ˜æ¡†æ¶ä½¿ç”¨å¼‚å¸¸æ•è·å¯èƒ½è¿˜æ˜¯ä¼šæŠ¥é”™ï¼ˆä¹Ÿå¯èƒ½ä¸æŠ¥é”™ï¼Œç¬¬ä¸€æ¬¡ç¬¬äºŒæ¬¡çˆ¬å–æ²¡æœ‰æŠ¥é”™ï¼‰ï¼Œï¼æ‰€ä»¥ä¸è¦ä½¿ç”¨å¼‚å¸¸æ•è·                                     
            UnboundLocalError: local variable 'introduction' referenced before assignment 
                                           
            èƒ½ä½¿ç”¨ä¸€äº›ä¾‹å¦‚å¼‚å¸¸é‡è¯•ä¹‹ç±»çš„æ–¹æ³•
            '''
            # try:
            introduction = div.css('.gy_box_txt>p:nth-child(2)>a::text').get()
            # åˆ¤æ–­ä¸€ä¸‹introductionæ˜¯ä¸æ˜¯ç©ºå€¼ï¼Œä¸æ˜¯è¿›è¡Œå­—ç¬¦ä¸²æ“ä½œï¼Œæ˜¯å°±ç»™introduction èµ‹ä¸€ä¸ªnoneå­—ç¬¦ä¸²
            if introduction:
                introduction = div.css('.gy_box_txt>p:nth-child(2)>a::text').get().strip().replace(' "', "'").replace(
                    ',', 'ï¼Œ').replace('"', "'")
            else:
                introduction = 'None'

            # except Exception:
            #     pass
            '''
            è¿™é‡Œå†™aæ ‡ç­¾å®šä½ä¸åˆ°ï¼Œä¸çŸ¥é“ä¸ºä»€ä¹ˆ --> èƒ½å®šä½åˆ°ï¼Œå’Œxpathè®°æ··æ·†äº†ï¼ŒxpathäºŒæ¬¡æå–è¦ç”¨.è¡¨ç¤ºä»å½“å‰èŠ‚ç‚¹å¼€å§‹, cssä¸å­˜åœ¨è¿™ä¸ªé—®é¢˜
            '''
            img = div.css('a>img::attr(src)').get()

            '''
            scrapy crawl language -o data.csv åœ¨æ²¡æœ‰å†™pipeline.pyä¿å­˜é€»è¾‘çš„æ—¶å€™ç”¨æ¥åšæµ‹è¯•ï¼ˆæµ‹è¯•è§£æçš„å­—æ®µæ˜¯å¦å®Œæ•´ï¼‰ï¼Œç”Ÿæˆçš„csvæ–‡ä»¶ä¼šæœ‰fieldï¼Œ
            æ­£å¼çš„ä¿å­˜æ–‡ä»¶ä¾‹å¦‚language.csvä¸ä¼šæœ‰field
            '''
            '''æ–¹æ³•ä¸€'''
            # yield {'title': title,
            #        'introduction': introduction,
            #        'img': img}
            '''æ–¹æ³•äºŒ'''
            yield ChinadailyItem(title=title, introduction=introduction, img=img)

        '''
        ï¼ï¼ï¼ï¼ï¼æˆ‘æœäº†ï¼Œæ ‡ç­¾ä¹‹å‰å®šä½æœ‰ä¸¤ä¸ªï¼Œæ•°æ®çˆ¬ä¸å®Œæ•´ï¼Œä¸€å®šè¦æ³¨æ„æ‰¾åˆ°å”¯ä¸€çš„æ ‡ç­¾å•Šï¼ï¼ï¼ï¼ï¼ï¼
        '''
        '''
        è¿™é‡Œå› ä¸ºnext_pageï¼ˆhrefï¼‰å¾—åˆ°æ²¡æœ‰https://æ‰€ä»¥è¦æå–å‡ºæ¥
        '''
        next_page = response.css('.content_left>#div_currpage>a:nth-last-child(2)::attr(href)').get().split('/')[-1]
        page_num = int(re.findall('.*_(.*?)\..*', next_page)[0])
        '''
        ï¼å¯ä»¥å­—ç¬¦ä¸²æ•°å­—æ¯”å¤§å° ï¼ˆ å‰é¢éªŒè¯ç å«ä¼ å­—ç¬¦ä¸²æ•°å­—ï¼Œä½†ä¼ intæ•°å­—ä¹Ÿå¯ä»¥ï¼Œ--> pythonå¥½åƒå¯¹äºæ•°å­—ä¸åŒºåˆ†æ˜¯strè¿˜æ˜¯int
        ---->>>>>>> !! è‰¹ï¼Œä¸èƒ½æ¯”è¾ƒå­—ç¬¦ä¸²æ•°å­—å¤§å° '2' < '11' å¾—åˆ°çš„æ˜¯False
        '''

        if page_num:
            url = 'http://language.chinadaily.com.cn/thelatest/' + next_page

            yield scrapy.Request(url=url, callback=self.parse)


'''
ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼
crawl æ‰€æœ‰æ•°æ®ï¼ˆè¿™é‡Œæ˜¯ä¸€é¡µä¸€é¡µçˆ¬å–ï¼Œå› ä¸ºä¸çŸ¥é“å¤šå°‘é¡µï¼Œæ‰¾ç¿»é¡µé€»è¾‘ä¸€é¡µä¸€é¡µç¿»çš„ï¼‰
çˆ¬å¤šäº†å‡ æ¬¡å°±ä¼šå˜æ…¢ï¼Œå¯èƒ½è¢«é™åˆ¶è¯·æ±‚äº†ï¼ˆä¹Ÿå¯èƒ½æ˜¯ç½‘é€Ÿé—®é¢˜ï¼Œç½‘é€Ÿè‡ªå·±æ„Ÿè§‰å¾—åˆ°ï¼Œä¹Ÿå¯ä»¥æµ‹é€Ÿï¼Œæ‰“å¼€è¯·æ±‚çš„ç½‘é¡µé€Ÿåº¦æ­£å¸¸ ç½‘é€Ÿä¸€èˆ¬å°±æ²¡æœ‰é—®é¢˜ï¼‰ï¼Œæˆ‘è®°å¾—åˆšå¼€å§‹å‡ åˆ†é’Ÿå°±çˆ¬å®Œäº†

INFO: Ignoring response <502 http://language.chinadaily.com.cn/thelatest/page_357.html>: HTTP status code is not handled or not allowed
2023-02-17 16:55:04 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <502 http://language.chinadaily.com.cn/thelatest/page_22.html>: HTTP status code is not handled or not allowed
æœ€ç»ˆçˆ¬åˆ°ä¸€å®šé¡µé¢å°±ä¸è®©çˆ¬å–æ•°æ®äº†ï¼Œè¿™æ ·åº”è¯¥æ˜¯è¢«è¯†åˆ«å‡ºæ¥ä¸æ˜¯äººæ­£å¸¸è¯·æ±‚ç»™åçˆ¬äº†ï¼Œåº”è¯¥è¦åŠ ä¸Šheaders cookiesæ‰èƒ½çˆ¬åˆ°äº†ï¼Œå­¦äº†scrapy é«˜çº§æ·»åŠ ä¸Šå†è¯•ä¸€ä¸‹

--->>> è¿™é‡Œæ˜¯çˆ¬å–é€Ÿåº¦å¤ªå¿«è¢«ğŸš«è¯·æ±‚äº† åœ¨scrapy default_settingå½“ä¸­è®¾ç½®äº† DOWNLOAD_DELAY=1ï¼ˆç­‰å¾…ä¸€ç§’å†è¿›è¡Œä¸‹ä¸€æ¬¡è¯·æ±‚ï¼Œä¹‹å‰é»˜è®¤æ˜¯=0ï¼‰
'''
